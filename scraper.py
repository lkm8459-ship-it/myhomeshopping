import requests
from bs4 import BeautifulSoup
import json
import re
import os

def scrape_fmkorea():
    url = "https://www.fmkorea.com/hotdeal"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
        'Referer': 'https://www.fmkorea.com/'
    }
    deals = []
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.select('div.fm_best_widget > ul > li')
        
        for item in items:
            title_node = item.select_one('h3.title > a')
            if not title_node: continue
            
            title_text = title_node.get_text(strip=True)
            title_text = re.sub(r'\[\d+\]$', '', title_text) 
            
            link = "https://www.fmkorea.com" + title_node['href']
            
            # Image (Lazy load support)
            img_node = item.select_one('img.thumb')
            img_url = ""
            if img_node:
                img_url = img_node.get('data-original') or img_node.get('data-src') or img_node.get('src') or ""
                if "transparent.gif" in img_url:
                    img_url = img_node.get('data-original') or img_node.get('data-src') or ""
                if img_url.startswith('//'):
                    img_url = "https:" + img_url
            
            if not img_url or "transparent.gif" in img_url:
                img_url = "https://via.placeholder.com/150/f1f5f9/4f46e5?text=FM"

            # Category
            cat_node = item.select_one('span.category > a')
            category = cat_node.get_text(strip=True).replace('/', '') if cat_node else "ê¸°íƒ€"
            
            # Price
            price = "ê°€ê²© ì •ë³´ ì—†ìŒ"
            info_div = item.select_one('div.hotdeal_info')
            if info_div:
                for span in info_div.find_all('span'):
                    if 'ê°€ê²©:' in span.get_text():
                        astrong = span.select_one('a.strong')
                        if astrong: price = astrong.get_text(strip=True)
                        break
            
            deals.append({
                "title": title_text,
                "price": price,
                "link": link,
                "img": img_url,
                "source": "FM Korea",
                "category": category,
                "isViral": any(word in title_text for word in ["ì—­ëŒ€ê¸‰", "í˜œíƒê°€", "ì§€ë¦¼", "ëŒ€ë°•"]),
                "date": "ì˜¤ëŠ˜"
            })
    except Exception as e:
        print(f"FM Korea Scrape Error: {e}")
    return deals[:12]

def scrape_ppomppu():
    url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
    }
    deals = []
    try:
        response = requests.get(url, headers=headers)
        # Try different decoding if EUC-KR fails
        try:
            content = response.content.decode('euc-kr')
        except:
            content = response.text
            
        soup = BeautifulSoup(content, 'html.parser')
        
        # Ppomppu items are in <tr> with class list0 or list1
        rows = soup.select('tr.list0, tr.list1')
        
        for row in rows:
            # Skip notice items (usually have 'notice' in img src or specific text)
            if row.select_one('img[src*="notice"]'): continue
            
            title_a = row.select_one('a.baseList-title')
            if not title_a: continue
            
            title_text = title_a.get_text(strip=True)
            title_text = re.sub(r'\[\d+\]$', '', title_text)
            
            link = "https://www.ppomppu.co.kr/zboard/" + title_a['href']
            
            # Image
            img_node = row.select_one('.baseList-thumb img')
            img_url = ""
            if img_node:
                img_url = img_node.get('src') or ""
                if img_url.startswith('//'): img_url = "https:" + img_url
            
            if not img_url:
                img_url = "https://via.placeholder.com/150/f1f5f9/4f46e5?text=PP"

            # Category
            cat_node = row.select_one('span.baseList-c')
            category = cat_node.get_text(strip=True).strip('[]') if cat_node else "ê¸°íƒ€"
            
            # Price
            price_match = re.search(r'\(([^)]+)\)', title_text)
            price = price_match.group(1) if price_match else "ê°€ê²© í™•ì¸"
            
            deals.append({
                "title": title_text,
                "price": price,
                "link": link,
                "img": img_url,
                "source": "Ppomppu",
                "category": category,
                "isViral": False,
                "date": "ì˜¤ëŠ˜"
            })
            if len(deals) >= 12: break
    except Exception as e:
        print(f"Ppomppu Scrape Error: {e}")
    return deals

def scrape_arcalive():
    url = "https://arca.live/b/hotdeal"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
    }
    deals = []
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì•„ì¹´ë¼ì´ë¸Œ í•«ë”œ ê²Œì‹œíŒ ë¦¬ìŠ¤íŠ¸
        items = soup.select('a.vrow:not(.notice)')
        
        for item in items:
            title_node = item.select_one('.title')
            if not title_node: continue
            
            title_text = title_node.get_text(strip=True)
            # ì¹´í…Œê³ ë¦¬ íƒœê·¸ í…ìŠ¤íŠ¸ ì§€ìš°ê¸° (ì˜ˆ: [PC/í•˜ë“œì›¨ì–´])
            cat_node = item.select_one('.badge')
            category = "ê¸°íƒ€"
            if cat_node:
                category = cat_node.get_text(strip=True)
                title_text = title_text.replace(category, '').strip()
            
            # ê°€ê²© ì¶”ì¶œ ë¡œì§ (ë³´í†µ ì œëª© ì•ˆì— ìˆìŒ)
            price = "í™•ì¸í•„ìš”"
            price_match = re.search(r'\(([\d,]+ì›)\)', title_text)
            if price_match:
                price = price_match.group(1)
            else:
                 price_match = re.search(r'([\d,]+ì›)', title_text)
                 if price_match: price = price_match.group(1)

            link = "https://arca.live" + item['href']
            
            deals.append({
                "title": title_text,
                "price": price,
                "link": link,
                "img": "https://via.placeholder.com/150/ff9900/ffffff?text=ARCA", # ë³´ìˆ˜ì  ì´ë¯¸ì§€
                "source": "Arcalive",
                "category": category,
                "isViral": False,
                "date": "ì˜¤ëŠ˜"
            })
            if len(deals) >= 15: break
            
    except Exception as e:
        print(f"Arcalive Scrape Error: {e}")
    return deals

def scrape_eomisae():
    url = "https://eomisae.co.kr/os"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    deals = []
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        items = soup.select('li.clear[class*="cx"]') # ì–´ë¯¸ìƒˆ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
        
        for item in items:
            title_node = item.select_one('a.hx')
            if not title_node: continue
            
            title_text = title_node.get_text(strip=True)
            link = "https://eomisae.co.kr" + title_node['href']
            
            price = "í™•ì¸í•„ìš”"
            
            deals.append({
                "title": title_text,
                "price": price,
                "link": link,
                "img": "https://via.placeholder.com/150/222222/ffffff?text=EOMI",
                "source": "Eomisae",
                "category": "íŒ¨ì…˜",
                "isViral": "í’ˆì ˆ" in title_text or "ë§‰ì°¨" in title_text,
                "date": "ì˜¤ëŠ˜"
            })
            if len(deals) >= 10: break
    except Exception as e:
         print(f"Eomisae Scrape Error: {e}")
    return deals


def main():
    print("ìˆ˜ì§‘ ì—”ì§„ ê°€ë™ ì¤‘ (ì´ë¯¸ì§€ ì •ë°€ íŒŒì‹±)...")
    all_deals = []
    
    # ë½ë¿Œ ìˆ˜ì§‘
    try:
        ppomppu_deals = scrape_ppomppu()
        all_deals.extend(ppomppu_deals)
        print(f"Ppomppu: {len(ppomppu_deals)}ê°œ")
    except Exception as e: 
        print(f"Ppomppu skip: {e}")
    
    # FMì½”ë¦¬ì•„ ìˆ˜ì§‘
    try:
        fm_deals = scrape_fmkorea()
        all_deals.extend(fm_deals)
        print(f"FM Korea: {len(fm_deals)}ê°œ")
    except Exception as e:
        print(f"FM Korea skip: {e}")
    
    # ì•„ì¹´ë¼ì´ë¸Œ ìˆ˜ì§‘
    try:
        arca_deals = scrape_arcalive()
        all_deals.extend(arca_deals)
        print(f"Arcalive: {len(arca_deals)}ê°œ")
    except Exception as e:
        print(f"Arca skip: {e}")
    
    # ì–´ë¯¸ìƒˆ ìˆ˜ì§‘
    try:
        eomi_deals = scrape_eomisae()
        all_deals.extend(eomi_deals)
        print(f"Eomisae: {len(eomi_deals)}ê°œ")
    except Exception as e:
        print(f"Eomi skip: {e}")
    
    # 1. ë¡œì»¬ ë°±ì—… ì €ì¥
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(all_deals, f, ensure_ascii=False, indent=2)
    
    # 2. Firebase ì‹¤ì‹œê°„ ì—…ë¡œë“œ (ìŠ¤ë§ˆíŠ¸í° ì•± ì—°ë™ìš©)
    FIREBASE_URL = "https://myhomeshopping-a9724-default-rtdb.firebaseio.com/deals.json"
    try:
        print("Firebase ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ì¤‘...")
        response = requests.put(FIREBASE_URL, json=all_deals)
        if response.status_code == 200:
            print("ğŸ‰ Firebase ì—…ë¡œë“œ ì™„ë£Œ! ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ì¡°íšŒê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print(f"Firebase ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
    except Exception as e:
        print(f"Firebase í†µì‹  ì˜¤ë¥˜: {e}")

    print(f"ëª¨ë“  ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(all_deals)}ê°œ)")

if __name__ == "__main__":
    main()
